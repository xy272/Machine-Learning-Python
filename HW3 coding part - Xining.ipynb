{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 1. KNN and Spam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset consists of a collection of 57 features relating to about 4600 emails and a label of whether or not the email is considered spam. You have a training set containing about 70% of the data and a test set containing about 30% of the data. Your job is to build effective spam classification rules using the predictors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note about features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The column names (in the first row of each .csv file) are fairly self-explanatory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables are named `word_freq_(word)`, which suggests a calculation of the frequency of how many times a specific word appears in the email, expressed as a percentage of total words in the email multiplied by $100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables are named `char_freq_(*)`, which suggests a count of the frequency of the specific ensuing character, expressed as a percentage of total characters in the email multiplied by $100$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some variables are named `capital_run_length_(*)` which suggests some information about the average (or maximum length of, or total) consecutive capital letters in the email."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`spam`: This is the response variable, 0 = not spam, 1 = spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dftr = pd.read_csv('spam_train_withlabels.csv',header=0)\n",
    "dfte = pd.read_csv('spam_test_nolabels.csv',header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.449</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.150</td>\n",
       "      <td>11</td>\n",
       "      <td>43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.204</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.285</td>\n",
       "      <td>2</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.55</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.087</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>47</td>\n",
       "      <td>94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.345</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.666</td>\n",
       "      <td>6</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.59</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.427</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>10.000</td>\n",
       "      <td>33</td>\n",
       "      <td>170</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0             0.0                0.0           0.00           0.0   \n",
       "1             0.0                0.0           0.00           0.0   \n",
       "2             0.0                0.0           0.55           0.0   \n",
       "3             0.0                0.0           0.00           0.0   \n",
       "4             0.0                0.0           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0            0.9             0.0              0.00                 0.0   \n",
       "1            0.0             0.0              0.00                 0.0   \n",
       "2            0.0             0.0              0.00                 0.0   \n",
       "3            0.0             0.0              0.00                 0.0   \n",
       "4            0.0             0.0              0.59                 0.0   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
       "0              0.0             0.0  ...                   0.0        0.000   \n",
       "1              0.0             0.0  ...                   0.0        1.204   \n",
       "2              0.0             0.0  ...                   0.0        0.000   \n",
       "3              0.0             0.0  ...                   0.0        0.000   \n",
       "4              0.0             0.0  ...                   0.0        0.000   \n",
       "\n",
       "   char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0        0.449          0.0        0.000          0.0          0.0   \n",
       "1        0.000          0.0        0.000          0.0          0.0   \n",
       "2        0.087          0.0        0.000          0.0          0.0   \n",
       "3        0.345          0.0        0.000          0.0          0.0   \n",
       "4        0.000          0.0        0.427          0.0          0.0   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       2.150                          11   \n",
       "1                       1.285                           2   \n",
       "2                         NaN                          47   \n",
       "3                       1.666                           6   \n",
       "4                      10.000                          33   \n",
       "\n",
       "   capital_run_length_total  \n",
       "0                        43  \n",
       "1                         9  \n",
       "2                        94  \n",
       "3                        55  \n",
       "4                       170  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfte.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spam</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.793</td>\n",
       "      <td>12</td>\n",
       "      <td>391</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-500.000</td>\n",
       "      <td>10</td>\n",
       "      <td>180</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.306</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.027</td>\n",
       "      <td>91</td>\n",
       "      <td>217</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.160</td>\n",
       "      <td>107</td>\n",
       "      <td>289</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.347</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00                0.1           0.00           0.0   \n",
       "1            0.00                0.0           0.29           0.0   \n",
       "2            0.47                0.0           0.95           0.0   \n",
       "3            0.00                0.0           0.37           0.0   \n",
       "4            0.00                0.0           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.10            0.21              0.00                0.10   \n",
       "1           0.00            0.00              0.00                0.00   \n",
       "2           0.95            0.00              0.95                0.00   \n",
       "3           1.11            0.74              0.00                2.96   \n",
       "4           0.00            0.00              0.00                0.00   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0              0.0            0.00  ...        0.000        0.047   \n",
       "1              0.0            0.00  ...        0.000        0.178   \n",
       "2              0.0            0.00  ...        0.000        0.076   \n",
       "3              0.0            2.96  ...        0.000        0.149   \n",
       "4              0.0            0.00  ...        0.634        0.000   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0        0.000        0.000        0.031          0.0   \n",
       "1        0.000        0.044        0.000          0.0   \n",
       "2        0.000        1.306        0.230          0.0   \n",
       "3        0.000        1.096        0.000          0.0   \n",
       "4        0.211        0.000        0.211          0.0   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       1.793                          12   \n",
       "1                    -500.000                          10   \n",
       "2                       6.027                          91   \n",
       "3                       5.160                         107   \n",
       "4                       1.347                           4   \n",
       "\n",
       "   capital_run_length_total  spam  \n",
       "0                       391     0  \n",
       "1                       180     0  \n",
       "2                       217     1  \n",
       "3                       289     1  \n",
       "4                        31     0  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dftr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_train_test = pd.concat([dftr.drop('spam',axis=1),dfte])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.10</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.793</td>\n",
       "      <td>12</td>\n",
       "      <td>391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.29</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.178</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.044</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-500.000</td>\n",
       "      <td>10</td>\n",
       "      <td>180</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.95</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.076</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.306</td>\n",
       "      <td>0.230</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.027</td>\n",
       "      <td>91</td>\n",
       "      <td>217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.37</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.11</td>\n",
       "      <td>0.74</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.96</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.96</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.149</td>\n",
       "      <td>0.000</td>\n",
       "      <td>1.096</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.160</td>\n",
       "      <td>107</td>\n",
       "      <td>289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.634</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.211</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.347</td>\n",
       "      <td>4</td>\n",
       "      <td>31</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00                0.1           0.00           0.0   \n",
       "1            0.00                0.0           0.29           0.0   \n",
       "2            0.47                0.0           0.95           0.0   \n",
       "3            0.00                0.0           0.37           0.0   \n",
       "4            0.00                0.0           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.10            0.21              0.00                0.10   \n",
       "1           0.00            0.00              0.00                0.00   \n",
       "2           0.95            0.00              0.95                0.00   \n",
       "3           1.11            0.74              0.00                2.96   \n",
       "4           0.00            0.00              0.00                0.00   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  word_freq_conference  char_freq_;  \\\n",
       "0              0.0            0.00  ...                   0.0        0.000   \n",
       "1              0.0            0.00  ...                   0.0        0.000   \n",
       "2              0.0            0.00  ...                   0.0        0.000   \n",
       "3              0.0            2.96  ...                   0.0        0.000   \n",
       "4              0.0            0.00  ...                   0.0        0.634   \n",
       "\n",
       "   char_freq_(  char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0        0.047        0.000        0.000        0.031          0.0   \n",
       "1        0.178        0.000        0.044        0.000          0.0   \n",
       "2        0.076        0.000        1.306        0.230          0.0   \n",
       "3        0.149        0.000        1.096        0.000          0.0   \n",
       "4        0.000        0.211        0.000        0.211          0.0   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       1.793                          12   \n",
       "1                    -500.000                          10   \n",
       "2                       6.027                          91   \n",
       "3                       5.160                         107   \n",
       "4                       1.347                           4   \n",
       "\n",
       "   capital_run_length_total  \n",
       "0                       391  \n",
       "1                       180  \n",
       "2                       217  \n",
       "3                       289  \n",
       "4                        31  \n",
       "\n",
       "[5 rows x 57 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many repeat indices are there in the merged dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "880"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(np.unique(merged_train_test.index,return_counts=True)[1]>1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values and outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can view missing values with the following code. Recall that an NA or `np.nan` represent missing values. **DO NOT DELETE THEM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "capital_run_length_average    404\n",
       "capital_run_length_total        0\n",
       "word_freq_report                0\n",
       "word_freq_hpl                   0\n",
       "word_freq_hp                    0\n",
       "word_freq_money                 0\n",
       "word_freq_000                   0\n",
       "word_freq_font                  0\n",
       "word_freq_your                  0\n",
       "word_freq_credit                0\n",
       "word_freq_you                   0\n",
       "word_freq_email                 0\n",
       "word_freq_business              0\n",
       "word_freq_free                  0\n",
       "word_freq_addresses             0\n",
       "word_freq_people                0\n",
       "word_freq_650                   0\n",
       "word_freq_will                  0\n",
       "word_freq_receive               0\n",
       "word_freq_mail                  0\n",
       "word_freq_order                 0\n",
       "word_freq_internet              0\n",
       "word_freq_remove                0\n",
       "word_freq_over                  0\n",
       "word_freq_our                   0\n",
       "word_freq_3d                    0\n",
       "word_freq_all                   0\n",
       "word_freq_address               0\n",
       "word_freq_george                0\n",
       "word_freq_lab                   0\n",
       "capital_run_length_longest      0\n",
       "word_freq_original              0\n",
       "char_freq_#                     0\n",
       "char_freq_$                     0\n",
       "char_freq_!                     0\n",
       "char_freq_[                     0\n",
       "char_freq_(                     0\n",
       "char_freq_;                     0\n",
       "word_freq_conference            0\n",
       "word_freq_table                 0\n",
       "word_freq_edu                   0\n",
       "word_freq_re                    0\n",
       "word_freq_project               0\n",
       "word_freq_meeting               0\n",
       "word_freq_labs                  0\n",
       "word_freq_cs                    0\n",
       "word_freq_direct                0\n",
       "word_freq_pm                    0\n",
       "word_freq_parts                 0\n",
       "word_freq_1999                  0\n",
       "word_freq_technology            0\n",
       "word_freq_85                    0\n",
       "word_freq_415                   0\n",
       "word_freq_data                  0\n",
       "word_freq_857                   0\n",
       "word_freq_telnet                0\n",
       "word_freq_make                  0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_train_test.isna().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 0\n",
    "Output the total number of outlier values that you have clearly found. Explain your reasoning. **DO NOT DELETE THEM**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>word_freq_conference</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4197.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.031869</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>-51.072045</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.285735</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>162.831083</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-500.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.333000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.076000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.401000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 57 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.065425   \n",
       "std          0.305358           1.290575       0.504143      1.395151   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     42.810000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail  ...  word_freq_conference  \\\n",
       "count      4601.000000     4601.000000  ...           4601.000000   \n",
       "mean          0.090067        0.239413  ...              0.031869   \n",
       "std           0.278616        0.644755  ...              0.285735   \n",
       "min           0.000000        0.000000  ...              0.000000   \n",
       "25%           0.000000        0.000000  ...              0.000000   \n",
       "50%           0.000000        0.000000  ...              0.000000   \n",
       "75%           0.000000        0.160000  ...              0.000000   \n",
       "max           5.260000       18.180000  ...             10.000000   \n",
       "\n",
       "       char_freq_;  char_freq_(  char_freq_[  char_freq_!  char_freq_$  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.038575     0.139030     0.016976     0.269071     0.075811   \n",
       "std       0.243471     0.270355     0.109394     0.815672     0.245882   \n",
       "min       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.065000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.188000     0.000000     0.315000     0.052000   \n",
       "max       4.385000     9.752000     4.081000    32.478000     6.003000   \n",
       "\n",
       "       char_freq_#  capital_run_length_average  capital_run_length_longest  \\\n",
       "count  4601.000000                 4197.000000                 4601.000000   \n",
       "mean      0.044238                  -51.072045                   52.172789   \n",
       "std       0.429342                  162.831083                  194.891310   \n",
       "min       0.000000                 -500.000000                    1.000000   \n",
       "25%       0.000000                    1.333000                    6.000000   \n",
       "50%       0.000000                    2.076000                   15.000000   \n",
       "75%       0.000000                    3.401000                   43.000000   \n",
       "max      19.829000                 1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total  \n",
       "count               4601.000000  \n",
       "mean                 283.289285  \n",
       "std                  606.347851  \n",
       "min                    1.000000  \n",
       "25%                   35.000000  \n",
       "50%                   95.000000  \n",
       "75%                  266.000000  \n",
       "max                15841.000000  \n",
       "\n",
       "[8 rows x 57 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##Histogram plots\n",
    "import matplotlib.pyplot as plt\n",
    "# Check statistics of each feature, and then plot the histogram of that feature, to see the number of outlier\n",
    "merged_train_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD4CAYAAAAAczaOAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8vihELAAAACXBIWXMAAAsTAAALEwEAmpwYAAAUAElEQVR4nO3df6zd9X3f8eerNiFeEwSMC3Nta3Yjt5phqimWR5dpYiUqLqlq8kckR1pgGpMjRKpk6zSZ5I8mf1gibdJ0aIOJNAzTpkFekwyUhDYEpcoiEZwLA4wBDzemcLGFbxtlIf+4s/PeH+dDObkc31/2PffSz/MhfXW+5/39fs73fc71ffn48/2e41QVkqQ+/MxyNyBJGh9DX5I6YuhLUkcMfUnqiKEvSR1ZvdwNzOWSSy6pjRs3LncbkvSW8vjjj/91VU3MrK/40N+4cSOTk5PL3YYkvaUk+atRdad3JKkjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpIyv+E7l669i452sLHvPi7e9dgk4knYnv9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JH5gz9JG9PciDJU0kOJflkq38iyStJnmzL9UNjbktyJMnhJNcN1a9KcrBtuyNJluZpSZJGmc91+ieBX62qHyc5D/hOkofats9W1aeHd06yBdgFXA78HPDNJL9QVaeBu4DdwHeBrwM7gIeQJI3FnO/0a+DH7e55balZhuwE7q+qk1V1FDgCbE+yFrigqh6tqgLuA244q+4lSQsyrzn9JKuSPAmcAB6uqsfapg8neTrJPUkuarV1wMtDw6dabV1bn1mXJI3JvEK/qk5X1VZgPYN37VcwmKp5F7AVOA58pu0+ap6+Zqm/SZLdSSaTTE5PT8+nRUnSPCzo6p2q+iHwF8COqnq1/WXwE+BzwPa22xSwYWjYeuBYq68fUR91nLuraltVbZuYmFhIi5KkWczn6p2JJBe29TXAe4Dn2xz9694HPNPWHwR2JTk/ySZgM3Cgqo4DryW5ul21cyPwwLl7KpKkuczn6p21wL4kqxj8JbG/qr6a5I+SbGUwRfMi8CGAqjqUZD/wLHAKuLVduQNwC3AvsIbBVTteuSNJYzRn6FfV08CVI+ofnGXMXmDviPokcMUCe5QknSN+IleSOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR2ZM/STvD3JgSRPJTmU5JOtfnGSh5O80G4vGhpzW5IjSQ4nuW6oflWSg23bHUmyNE9LkjTKfN7pnwR+tap+CdgK7EhyNbAHeKSqNgOPtPsk2QLsAi4HdgB3JlnVHusuYDewuS07zt1TkSTNZc7Qr4Eft7vntaWAncC+Vt8H3NDWdwL3V9XJqjoKHAG2J1kLXFBVj1ZVAfcNjZEkjcG85vSTrEryJHACeLiqHgMuq6rjAO320rb7OuDloeFTrbaurc+sjzre7iSTSSanp6cX8HQkSbOZV+hX1emq2gqsZ/Cu/YpZdh81T1+z1Ecd7+6q2lZV2yYmJubToiRpHhZ09U5V/RD4CwZz8a+2KRva7Ym22xSwYWjYeuBYq68fUZckjcl8rt6ZSHJhW18DvAd4HngQuKntdhPwQFt/ENiV5PwkmxicsD3QpoBeS3J1u2rnxqExkqQxWD2PfdYC+9oVOD8D7K+qryZ5FNif5GbgJeD9AFV1KMl+4FngFHBrVZ1uj3ULcC+wBnioLZKkMZkz9KvqaeDKEfW/Aa49w5i9wN4R9UlgtvMBkqQl5CdyJakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI7MGfpJNiT5VpLnkhxK8pFW/0SSV5I82Zbrh8bcluRIksNJrhuqX5XkYNt2R5IszdOSJI0y53+MDpwCfruqnkjyTuDxJA+3bZ+tqk8P75xkC7ALuBz4OeCbSX6hqk4DdwG7ge8CXwd2AA+dm6ciSZrLnO/0q+p4VT3R1l8DngPWzTJkJ3B/VZ2sqqPAEWB7krXABVX1aFUVcB9ww9k+AUnS/C1oTj/JRuBK4LFW+nCSp5Pck+SiVlsHvDw0bKrV1rX1mfVRx9mdZDLJ5PT09EJalCTNYt6hn+QdwJeAj1bVjxhM1bwL2AocBz7z+q4jhtcs9TcXq+6uqm1VtW1iYmK+LUqS5jCv0E9yHoPA/0JVfRmgql6tqtNV9RPgc8D2tvsUsGFo+HrgWKuvH1GXJI3JfK7eCfB54Lmq+v2h+tqh3d4HPNPWHwR2JTk/ySZgM3Cgqo4DryW5uj3mjcAD5+h5SJLmYT5X77wb+CBwMMmTrfYx4ANJtjKYonkR+BBAVR1Ksh94lsGVP7e2K3cAbgHuBdYwuGrHK3ckaYzmDP2q+g6j5+O/PsuYvcDeEfVJ4IqFNChJOnf8RK4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUkTlDP8mGJN9K8lySQ0k+0uoXJ3k4yQvt9qKhMbclOZLkcJLrhupXJTnYtt2RZNT/vStJWiLzead/CvjtqvonwNXArUm2AHuAR6pqM/BIu0/btgu4HNgB3JlkVXusu4DdwOa27DiHz0WSNIc5Q7+qjlfVE239NeA5YB2wE9jXdtsH3NDWdwL3V9XJqjoKHAG2J1kLXFBVj1ZVAfcNjZEkjcGC5vSTbASuBB4DLquq4zD4iwG4tO22Dnh5aNhUq61r6zPro46zO8lkksnp6emFtChJmsW8Qz/JO4AvAR+tqh/NtuuIWs1Sf3Ox6u6q2lZV2yYmJubboiRpDvMK/STnMQj8L1TVl1v51TZlQ7s90epTwIah4euBY62+fkRdkjQm87l6J8Dngeeq6veHNj0I3NTWbwIeGKrvSnJ+kk0MTtgeaFNAryW5uj3mjUNjJEljsHoe+7wb+CBwMMmTrfYx4HZgf5KbgZeA9wNU1aEk+4FnGVz5c2tVnW7jbgHuBdYAD7VFkjQmc4Z+VX2H0fPxANeeYcxeYO+I+iRwxUIalCSdO34iV5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SerInKGf5J4kJ5I8M1T7RJJXkjzZluuHtt2W5EiSw0muG6pfleRg23ZHkjP9v7uSpCUyn3f69wI7RtQ/W1Vb2/J1gCRbgF3A5W3MnUlWtf3vAnYDm9sy6jElSUtoztCvqm8DP5jn4+0E7q+qk1V1FDgCbE+yFrigqh6tqgLuA25YZM+SpEU6mzn9Dyd5uk3/XNRq64CXh/aZarV1bX1mfaQku5NMJpmcnp4+ixYlScMWG/p3Ae8CtgLHgc+0+qh5+pqlPlJV3V1V26pq28TExCJblCTNtKjQr6pXq+p0Vf0E+BywvW2aAjYM7boeONbq60fUJUljtKjQb3P0r3sf8PqVPQ8Cu5Kcn2QTgxO2B6rqOPBakqvbVTs3Ag+cRd+SpEVYPdcOSb4IXANckmQK+B3gmiRbGUzRvAh8CKCqDiXZDzwLnAJurarT7aFuYXAl0BrgobZIksZoztCvqg+MKH9+lv33AntH1CeBKxbUnSTpnPITuZLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjhj6ktQRQ1+SOmLoS1JH5gz9JPckOZHkmaHaxUkeTvJCu71oaNttSY4kOZzkuqH6VUkOtm13JMm5fzqSpNnM553+vcCOGbU9wCNVtRl4pN0nyRZgF3B5G3NnklVtzF3AbmBzW2Y+piRpic0Z+lX1beAHM8o7gX1tfR9ww1D9/qo6WVVHgSPA9iRrgQuq6tGqKuC+oTGSpDFZ7Jz+ZVV1HKDdXtrq64CXh/abarV1bX1mfaQku5NMJpmcnp5eZIuSpJnO9YncUfP0NUt9pKq6u6q2VdW2iYmJc9acJPVusaH/apuyod2eaPUpYMPQfuuBY62+fkRdkjRGiw39B4Gb2vpNwAND9V1Jzk+yicEJ2wNtCui1JFe3q3ZuHBojSRqT1XPtkOSLwDXAJUmmgN8Bbgf2J7kZeAl4P0BVHUqyH3gWOAXcWlWn20PdwuBKoDXAQ22RJI3RnKFfVR84w6Zrz7D/XmDviPokcMWCupMknVN+IleSOmLoS1JHDH1J6oihL0kdMfQlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyFmFfpIXkxxM8mSSyVa7OMnDSV5otxcN7X9bkiNJDie57myblyQtzLl4p/+vqmprVW1r9/cAj1TVZuCRdp8kW4BdwOXADuDOJKvOwfElSfO0FNM7O4F9bX0fcMNQ/f6qOllVR4EjwPYlOL4k6QzONvQL+EaSx5PsbrXLquo4QLu9tNXXAS8PjZ1qtTdJsjvJZJLJ6enps2xRkvS61Wc5/t1VdSzJpcDDSZ6fZd+MqNWoHavqbuBugG3bto3cZz427vnaosa9ePt7F3tISVrRzuqdflUda7cngK8wmK55NclagHZ7ou0+BWwYGr4eOHY2x5ckLcyiQz/JzyZ55+vrwK8BzwAPAje13W4CHmjrDwK7kpyfZBOwGTiw2ONLkhbubKZ3LgO+kuT1x/mTqvqzJN8D9ie5GXgJeD9AVR1Ksh94FjgF3FpVp8+qe0nSgiw69Kvq+8Avjaj/DXDtGcbsBfYu9piSpLPjJ3IlqSOGviR1xNCXpI4Y+pLUEUNfkjpi6EtSRwx9SeqIoS9JHTH0Jakjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqiKEvSR0x9CWpI4a+JHXE0Jekjow99JPsSHI4yZEke8Z9fEnq2VhDP8kq4L8Cvw5sAT6QZMs4e5Cknq0e8/G2A0eq6vsASe4HdgLPjrkPrTAb93xtUeNevP2957gT6e+3cYf+OuDloftTwD+buVOS3cDudvfHSQ6Pobc3jv+pN5UuAf56nD3M00rtC+bZ24jXekEWMX6lvmb2tTD2Nbd/PKo47tDPiFq9qVB1N3D30rczP0kmq2rbcvcx00rtC1Zub/a1MPa1MCu1r2HjPpE7BWwYur8eODbmHiSpW+MO/e8Bm5NsSvI2YBfw4Jh7kKRujXV6p6pOJfkw8OfAKuCeqjo0zh4WacVMNc2wUvuCldubfS2MfS3MSu3r76TqTVPqkqS/p/xEriR1xNCXpI4Y+meQ5D8mqSSXDNVua18fcTjJdUP1q5IcbNvuSDLq0tSz7ef3kjyf5OkkX0ly4Uroa0Sfy/Y1G0k2JPlWkueSHErykVa/OMnDSV5otxcNjRn52i1Rf6uS/O8kX10pfSW5MMmftj9bzyX5lRXS179vP8NnknwxyduXq68k9yQ5keSZodqCe1mO38eRqsplxsLgstI/B/4KuKTVtgBPAecDm4C/BFa1bQeAX2HwOYSHgF9fgp5+DVjd1j8FfGol9DWjx1Xt+D8PvK31tWWMP7e1wC+39XcC/6e9Pr8L7Gn1PfN57Zaov/8A/Anw1XZ/2fsC9gH/rq2/Dbhwufti8CHOo8Cadn8/8G+Wqy/gXwK/DDwzVFtwL+P+fTzT4jv90T4L/Cd++oNjO4H7q+pkVR0FjgDbk6wFLqiqR2vwk70PuOFcN1RV36iqU+3udxl8xmHZ+5rh775mo6r+Fnj9azbGoqqOV9UTbf014DkGAbKTQbjRbm9o6yNfu6XoLcl64L3AHw6Vl7WvJBcwCLTPA1TV31bVD5e7r2Y1sCbJauAfMPg8z7L0VVXfBn4wo7ygXpbp93EkQ3+GJL8JvFJVT83YNOorJNa1ZWpEfSn9WwbvFFZaX2fqZeySbASuBB4DLquq4zD4iwG4tO02zn7/gMEbiZ8M1Za7r58HpoH/3qad/jDJzy53X1X1CvBp4CXgOPB/q+oby93XDAvtZTl+H0ca99cwrAhJvgn8oxGbPg58jMFUypuGjajVLPVz2ldVPdD2+ThwCvjCuPpagOU45pubSN4BfAn4aFX9aJap07H0m+Q3gBNV9XiSa+YzZERtKV7H1QymLX6rqh5L8p8ZTFUsa19tfnwng+mRHwL/I8m/Xu6+5mkl/T6O1GXoV9V7RtWT/FMGf9CeakGxHngiyXbO/BUSU7wx1TJcP2d9DfV3E/AbwLXtn4iMo68FWPav2UhyHoPA/0JVfbmVX02ytqqOt39mn2j1cfX7buA3k1wPvB24IMkfr4C+poCpqnqs3f9TBqG/3H29BzhaVdMASb4M/PMV0NewhfayHL+Poy3HiYS3ygK8yBsnci/np0/QfJ83TtB8D7iaN07QXL8Evexg8BXUEzPqy9rXjF5Wt+Nv4o0TuZeP8ecVBnOlfzCj/nv89Em3353rtVvCHq/hjRO5y94X8L+AX2zrn2g9LWtfDL559xCDufwwmDP/reXsC9jIT5/IXXAv4/59PONzWY6DvlUWhkK/3f84g7Pxhxk68w5sA55p2/4L7ZPO57iXIwzmCp9sy39bCX2N6PN6BlfN/CWDaalx/rz+BYN/Mj899DpdD/xD4BHghXZ78Vyv3RL2eA1vhP6y9wVsBSbba/Y/gYtWSF+fBJ5vf37/qIXosvQFfJHBuYX/x+Ad+82L6WU5fh9HLX4NgyR1xKt3JKkjhr4kdcTQl6SOGPqS1BFDX5I6YuhLUkcMfUnqyP8HLRQuUSiKO1AAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# We could see that in the column 'capital_run_length_average' there exists losts of negative values, which can be outliers.\n",
    "# So, we plot the histogram as follows\n",
    "plt.hist(merged_train_test['capital_run_length_average'], bins=20, rwidth=0.9)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "470\n"
     ]
    }
   ],
   "source": [
    "# learn from the histogram, we define the negative data in column 'capital_run_length_average' as outliers. \n",
    "# And count the numbers\n",
    "n_outlier = merged_train_test[merged_train_test['capital_run_length_average'] < 0].shape[0]\n",
    "print(n_outlier)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split off the `spam` column **from the training dataframe** into a new vector called `ytrain` and drop the column from the training dataframe. Use $k$-nearest neighbors regression **trained on the training dataset** with $k = 15$ to impute the missing/outlier values in the `capital_run_length_average` column using the other predictors after standardizing (i.e. rescaling) them--there are times when one could train on both test and train, but let's focus on train for now c.f. inductive learning. You are allowed to just use the `sklearn` neighbors module to perform this. (Take a look at the pandas `concat` command and remember the `drop` command from the last HW.) Remember to rescale using information from the training set. Rescale the test set also.\n",
    "\n",
    "To recap, you are training a nearest neighbors regressor on the training set to predict the value of the `capital_run_length_average` variable. Then you will use that regressor to fill in the missing/outlier data of the same column, but in the **test** dataframe. To do this you will need to extract the part of the **train** dataset that has valid values for `capital_run_length_average` and then train your regressor on the valid dataset.\n",
    "\n",
    "To do the above you are rescaling the columns of the **train** dataset with the mean and standard deviation values from the **full train** dataset. Then you are using those same values to rescale the **test** dataset.\n",
    "\n",
    "Once you have done then, then you apply nearest neighbors regression and impute (fill in) the missing/outlier values of `capital_run_length_average`. Then again, you will standardize that column in both the **traininig** data set and the **test** dataset. That is remove the mean and divide by standard deviation that is found on the **training** set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-465e55f0423a>:6: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtrain['capital_run_length_average'][xtrain['capital_run_length_average'] < 0] = np.nan\n",
      "<ipython-input-13-465e55f0423a>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  xtest['capital_run_length_average'][xtest['capital_run_length_average'] < 0] = np.nan\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "# split off the spam column from the training dataframe\n",
    "ytrain = dftr['spam'].to_numpy()\n",
    "xtrain = dftr.drop('spam', axis=1)\n",
    "# set outliers in 'capital_run_length_average' to be nan as well\n",
    "xtrain['capital_run_length_average'][xtrain['capital_run_length_average'] < 0] = np.nan\n",
    "xtest = dfte\n",
    "xtest['capital_run_length_average'][xtest['capital_run_length_average'] < 0] = np.nan\n",
    "# standardizing\n",
    "m = np.nanmean(xtrain, axis=0)\n",
    "s = np.nanstd(xtrain, axis=0)\n",
    "xtrain_scale = (xtrain - m) / s\n",
    "xtest_scale = (xtest - m) / s\n",
    "# kNN regressor training\n",
    "xtrain_select = xtrain_scale[xtrain_scale['capital_run_length_average'].notnull()]\n",
    "vtrain_select = xtrain_select['capital_run_length_average']\n",
    "xtrain_select = xtrain_select.drop('capital_run_length_average', axis=1)\n",
    "neigh = KNeighborsRegressor(15)\n",
    "neigh.fit(xtrain_select, vtrain_select)\n",
    "# KNN regressor imputing NA's and outliers (replaced by np.nan already) in both training and testing dataframe\n",
    "xtest_select1 = xtrain_scale[~xtrain_scale['capital_run_length_average'].notnull()]\n",
    "xtest_select1 = xtest_select1.drop('capital_run_length_average', axis=1)\n",
    "xtrain_scale['capital_run_length_average'][~xtrain_scale['capital_run_length_average'].notnull()] = neigh.predict(xtest_select1)\n",
    "xtest_select2 = xtest_scale[~xtest_scale['capital_run_length_average'].notnull()]\n",
    "xtest_select2 = xtest_select2.drop('capital_run_length_average', axis=1)\n",
    "xtest_scale['capital_run_length_average'][~xtest_scale['capital_run_length_average'].notnull()] = neigh.predict(xtest_select2)\n",
    "# rescale again\n",
    "m = np.nanmean(xtrain_scale, axis=0)\n",
    "s = np.nanstd(xtrain_scale, axis=0)\n",
    "xtrain_scale = (xtrain_scale - m) / s\n",
    "xtest_scale = (xtest_scale - m) / s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xtrain_scale.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# assign back \n",
    "dftr = xtrain_scale\n",
    "dfte = xtest_scale\n",
    "ytr = ytrain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When you are done with this part, you should have no more NA’s in the capital_run_length_average column in either the training or the test set. The mean (standard deviation) of the training dataset should be 0 (resp 1). The test should should be close to that, but not exactly that since you are rescaling based on the training data. There are times when it might make sense to scale based on the testing data (like if you assume the distribution has shifted a little bit), but we aren't doing that. Make sure you show all of your work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function named `knnlearn()` that performs $k$-nearest neighbors classification along with some additional behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-  The function should automatically do a split of the training data into a sub-training set (80%) and a validation set (20%) for selecting the optimal $k$.\n",
    "- The function should standardize each column: for a particular variable, say `x1`, compute the mean and standard deviation of `x1` **using the training set only**, say `mu1` and `s1`; then for each observed column in the training set and test set, subtract the respective `mu` and divide by the respective `s`. Your data should already be standardized, but now this code does it automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knnlearn(xtrain,xtest,ytrain):\n",
    "    ntrain = xtrain.shape[0]\n",
    "    trainbools = np.array([True]*round(ntrain*0.8) + [False]*(ntrain - round(ntrain*0.8)))\n",
    "    np.random.shuffle(trainbools)\n",
    "    xtrain_train = xtrain.iloc[trainbools,:]\n",
    "    xtrain_test = xtrain.iloc[~trainbools,:]\n",
    "    \"\"\"\n",
    "    your code that standardizes and does nearest neighbors as well as picks the best k based on the xtrain_test behavior\n",
    "    you can use a package for the NN part, you've already implemented kNN in the previous homework\n",
    "    your code for rescaling must use broadcasting. you cannot use for loops\n",
    "    \"\"\"\n",
    "    m = np.mean(xtrain, axis=0)\n",
    "    s = np.std(xtrain, axis=0)\n",
    "    xtrain_scale = (xtrain - m) / s\n",
    "    xtrain_train_scale = xtrain_scale.iloc[trainbools,:]\n",
    "    xtrain_test_scale = xtrain_scale.iloc[~trainbools,:]\n",
    "    # xtrain_train_scale = (xtrain_train - m) / s\n",
    "    # xtrain_test_scale = (xtrain_test - m) / s\n",
    "    xtest_scale = (xtest - m) / s\n",
    "    # kNN\n",
    "    K = 50 # odd k from 1 to 29?\n",
    "    score = np.zeros(K)\n",
    "    for k in range(K):\n",
    "        k += 1\n",
    "        neigh = KNeighborsClassifier(k)\n",
    "        neigh.fit(xtrain_train_scale, ytrain[trainbools])\n",
    "        # yvalid = neigh.predict(xtrain_test_scale)\n",
    "        score[k-1] = neigh.score(xtrain_test_scale, ytrain[~trainbools])\n",
    "    k_hat = np.argmin(score) + 1\n",
    "\n",
    "    neigh = KNeighborsClassifier(k_hat)\n",
    "    neigh.fit(xtrain_scale, ytrain)\n",
    "    ytest = neigh.predict(xtest_scale)\n",
    "    return ytest\n",
    "    # return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "S = knnlearn(dftr,dfte,ytr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part, you will use your k-NN classifier to fit models on the actual dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now fit 2 models and produce 2 sets of predictions of spam on the test set:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `knnlearn()` using all predictors except for `capital_run_length_average` (say, if we were distrustful of our imputation approach). Call these predictions `knn_pred1`.\n",
    "\n",
    "2. `knnlearn()` using all predictors including `capital_run_length_average` with the imputed values. Call these predictions `knn_pred2`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit a `.csv` file called `assn2_NETID_results.csv` (to Canvas, **NOT** Gradescope) with the columns:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. `capital_run_length_average`: the predictor in your test set that now contains the imputed values (so that we can check your work on imputation).\n",
    "2. `knn_pred1`\n",
    "3. `knn_pred2`\n",
    "\n",
    "You can do so by creating a new pandas dataframe with those columns and then using `df.to_csv(filename,index=False)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure that row 1 here corresponds to row 1 of the test set, row 2 corresponds to row 2 of the test set, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain1 = dftr.drop('capital_run_length_average', axis=1)\n",
    "xtest1 = dfte.drop('capital_run_length_average', axis=1)\n",
    "knn_pred1 = knnlearn(xtrain1, xtest1, ytrain)\n",
    "\n",
    "xtrain2 = dftr\n",
    "xtest2 = dfte\n",
    "knn_pred2 = knnlearn(xtrain2, xtest2, ytrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save to csv\n",
    "d = {'capital_run_length_average':dfte['capital_run_length_average'].to_numpy(), 'knn_pred1':knn_pred1, 'knn_pred2':knn_pred2}\n",
    "df_result = pd.DataFrame(data=d)\n",
    "df_result.to_csv('./assn2_NETID_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem 2. Methods to solve Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We saw that the solution to least squares is $\\hat{\\beta} = (X^T X)^{-1} X^T y$. There is no such closed form solution for logistic regression. Recall that for logistic regression (when $y_i \\in \\{0,1\\}$)\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = \\arg \\min_{\\beta} f(\\beta)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "f(\\beta) = \\frac{1}{n} \\sum_{i=1}^n -y_i x_i^T \\beta + \\log(1+\\exp(x_i^T \\beta))\n",
    "$$\n",
    "Later on we will explore various methods to solve such problems. For now, one method is an interative method called Newton's method where we start with $\\beta_0 = 0$ and update based on\n",
    "$$\n",
    "\\beta_{k+1} = \\beta_k - [H f(\\beta_k)]^{-1} \\nabla f(\\beta_k)\n",
    "$$\n",
    "where\n",
    "$$\n",
    "[H f(\\beta_k)]_{ij} = \\frac{\\partial^2 f(\\beta_k)}{\\partial_i \\partial_j}\n",
    "$$\n",
    "is the matrix of second derivatives also known as the Hessian."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 1\n",
    "Write a function to implement 100 iterations of the above method that also outputs each iterate $\\beta_k$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logreg(X,y):\n",
    "    \"\"\"\n",
    "    input\n",
    "    X: 2-d numpy array that is n by p\n",
    "    y: numpy array of 0 and 1 of length n\n",
    "    output\n",
    "    B:\n",
    "    2-d numpy array that is p by 100 where the $i^{th}$ column is beta_i.\n",
    "    By definition the first (0^{th} in numpy) column of B should be all zeros.\n",
    "    \"\"\"\n",
    "    n,p = X.shape ##python unpackaging a tuple\n",
    "    assert n==y.shape[0]\n",
    "    # suppose y is column vector, convert if not\n",
    "    # y.ravel()\n",
    "    # y = y[:, np.newaxis]\n",
    "    ITER = 100\n",
    "    B = np.zeros((p,ITER))\n",
    "    for j in range(1,ITER):\n",
    "        beta = B[:,j-1] # define beta as previous value\n",
    "        p = 1/(1+np.exp(-X@beta)) ##this object will be useful\n",
    "        gradient = 0 ##one line of code here, no for loops, use broadcasting\n",
    "        gradient = 1/n * X.T @ (p - y) \n",
    "        Hessian = 0 ##one line of code here, no for loops, use broadcasting\n",
    "        Hessian = 1/n * X.T @ np.diag(p*(1-p)) @ X\n",
    "        B[:,j] = 0 ##one line of code here, no for loops, use broadcasting\n",
    "        # B[:,j] = beta - np.linalg.inv(Hessian) @ gradient\n",
    "        B[:,j] = beta - np.linalg.solve(Hessian, gradient) # try to avoid inverse\n",
    "    return B"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task 2\n",
    "Generate simulated data, so we can test the code. Take $p=5$ and $n=1000$. Plot the log of the estimation error for each column of `B`. Compute the estimation error as $v_k = \\| \\beta_k - \\beta^* \\|_2^2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn as sk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill in code here\n",
    "n=1000\n",
    "p=5\n",
    "betastar = np.ones(5)/np.sqrt(5)\n",
    "X = np.random.randn(n,p)\n",
    "# something_here_that_follows_logistic_regression_model\n",
    "np.random.seed(0)\n",
    "# y = np.random.binomial(1., 1/(1+np.exp(-X@betastar))) # logistic regression model, simulate the binary response vector from the Bernoulli distribution \n",
    "y = np.random.rand(n) < 1/(1+np.exp(-X@betastar))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "Biterates = logreg(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x267a47212b0>]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# v = squared errors\n",
    "v = np.linalg.norm(Biterates - betastar[:, np.newaxis], axis=0)\n",
    "plt.plot(np.log(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
